{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkrqqmkv5LOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPTfaXxO5eYy",
        "colab_type": "code",
        "outputId": "ed004e15-efc6-4bd2-df8c-155b96385663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lstm= nn.LSTM(3,3) # input dim and output dim =3\n",
        "\n",
        "inputs=[torch.randn(1,3) for _ in range(5)] # make a sequence of length 5\n",
        "hidden=(torch.randn(1,1,3), torch.randn(1,1,3)) #initialize a hidden state\n",
        "\n",
        "for i in inputs:\n",
        "  out, hidden=lstm(i.view(1,1,-1), hidden)\n",
        "\n",
        "inputs=torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden=(torch.randn(1, 1, 3), torch.randn(1, 1, 3)) # clear out hidden states\n",
        "\n",
        "out, hidden=(inputs, hidden)\n",
        "\n",
        "print(f\"Output: {out}\")\n",
        "print(f\"Hidden: {hidden}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output: tensor([[[ 0.2840, -0.5457, -0.7092]],\n",
            "\n",
            "        [[-0.0397,  0.6734,  0.2899]],\n",
            "\n",
            "        [[-0.6523,  0.4644, -0.7886]],\n",
            "\n",
            "        [[-0.1547, -0.6124, -1.1373]],\n",
            "\n",
            "        [[-1.9719,  2.4837, -0.2139]]])\n",
            "Hidden: (tensor([[[1.6444, 0.3410, 1.0874]]]), tensor([[[ 0.2515, -0.0496,  2.1962]]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9zqlsjV7eib",
        "colab_type": "code",
        "outputId": "5975c4c7-adeb-40c9-e988-d76dfccbf044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def prepare_sequence(sequence,to_ix):\n",
        "  idx=[to_ix[w] for w in sequence]\n",
        "  return torch.tensor(idx,dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data=[(\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "               (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "               ]\n",
        "\n",
        "word_to_ix={}\n",
        "\n",
        "for sent, tags in training_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word]=len(word_to_ix)\n",
        "\n",
        "print(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o8SrjoZ-2m0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  \n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "\n",
        "\n",
        "    self.hidden_size=hidden_dim\n",
        "    self.word_embeddings=nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm=nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "    self.hidden_to_tag= nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    embeds=self.word_embeddings(sentence)\n",
        "    lstm_out, _=self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "\n",
        "    tag_space=self.hidden_to_tag(lstm_out.view(len(sentence), -1))\n",
        "\n",
        "    tag_score=F.softmax(tag_space, dim=1)\n",
        "\n",
        "    return tag_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9o3Oap3BpSw",
        "colab_type": "code",
        "outputId": "7c27199c-d7b0-4475-e302-d28822e9a8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model=LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "\n",
        "loss_function=nn.NLLLoss()\n",
        "\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  inputs=prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  tag_score=model(inputs)\n",
        "  print(tag_score)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(30):\n",
        "  for sentence, tags in training_data:\n",
        "    model.zero_grad() # clear gradients since pytorch accumulate gradient\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    targets= prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "    tag_scores=model(sentence_in)\n",
        "\n",
        "    loss=loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "with torch.no_grad():\n",
        "  inputs=prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  tag_scores=model(inputs)\n",
        "  print(tag_scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2444, 0.3690, 0.3866],\n",
            "        [0.2258, 0.3422, 0.4320],\n",
            "        [0.2208, 0.3468, 0.4324],\n",
            "        [0.2387, 0.3381, 0.4232],\n",
            "        [0.2225, 0.3438, 0.4336]])\n",
            "tensor([[0.2408, 0.4800, 0.2792],\n",
            "        [0.2213, 0.4485, 0.3303],\n",
            "        [0.2140, 0.4557, 0.3303],\n",
            "        [0.2404, 0.4315, 0.3281],\n",
            "        [0.2176, 0.4568, 0.3256]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkMHJ8sXF4p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}